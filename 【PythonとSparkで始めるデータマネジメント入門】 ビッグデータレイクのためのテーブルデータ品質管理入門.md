# 【PythonとSparkで始めるデータマネジメント入門】 ビッグデータレイクのためのテーブルデータ品質管理入門

## セクション2: データ品質管理ことはじめ
* データマネジメント：データ利用のことを考えデータ利用の生産性向上を目的とする
* データ品質管理：データ自体をどうよくするかのサイクル
    * 三原則
        1. 防ぐ　　：組織でのルール作りやシステム開発
        2. 見つける：データの検知や可視化
        3. 修正する：データ修正やルールやシステムを修正
    * 三原則の比率
        1. 防ぐ　　：40%
        2. 見つける：40%
        3. 修正する：20%
    * データ品質テスト
        * ”見つける”の具体的な施策
        * 「バリデーション」や「単体テスト」に近い
    * データ品質を測定する6つの指標
        1. 正確性
        2. 有効性
        3. ユニーク性
        4. 適時性
        5. 完全生
        6. 一貫性
    * データの劣化パターン
        1. データ往来
        2. データの変換
        3. 時間の経過
        4. 人的要因

## セクション3: データ品質のテストとリペア
* テストレベルの設定
    * テーブル単位でのテスト
    * カラム単位でのテスト（一番多い）
    * テーブル間単位でのテスト
* 自身でデータを理解して定義を考えていく
    * 単体テストなどと似ている
* データの概要把握に使える
    * ```python
      df.summary().show()
      ```
### テストの種類
* if-thenテスト　　　　　：バリデーションチェック（数値の範囲など）
* ゼロコントロール　　　：四則演算の結果について確認（合計が正しいかなど）
* 辞書テスト　　　　　　：対象の項目が含まれるか
* レンジテスト　　　　　：対象の項目が適切な範囲か
* Nullチェック　　　　　　：nullのチェック
* ユニークネス　　　　　：重複チェック
* パターンチェック　　　：正規表現のチェック
* コンシステンシー　　　：結合できるかの確認（集合演算）
* レイショーコントロール：割合について極端な差がないかの確認
* タイムラインネス　　　：データが特定の時間に処理されているか確認
* メタデータのサジェスト：カラム名の一致度を算出して、同内容のカラムを検出
* 0件チェック　　　　　　：テーブル単位でレコード件数の確認
* カラム数チェック　　　：カラム数が急上昇などしていないか
* データのリペア　　　　：不要なデータを除く

## セクション4: メタデータとデータ品質の連携
