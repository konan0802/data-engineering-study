# 超速入門!【データサイエンスへの最初の一歩】PythonとSparkで学ぶデータ分析のための前処理と分散処理 一気見講座

## 【教材】
* [超速入門!【データサイエンスへの最初の一歩】PythonとSparkで学ぶデータ分析のための前処理と分散処理 一気見講座](https://www.udemy.com/course/spark-python-crush-course/)
* [PySparkの環境構築](https://github.com/yk-st/pyspark_settings)

## 【内容】
### セクション1: 紹介
* 環境構築
    * ローカルのメモリが少なかったためAWSのEC2上に構築
    * [`Docker.ipynb`](https://github.com/yk-st/pyspark_settings/blob/main/Docker.ipynb) 通りにPySpark環境を構築

### セクション2: 環境構築と基本操作（DataFrame）
7. 分散処理とは？
    * スレッド処理：一台のパソコンの中でスレッドを立ち上げ処理を行う
    * 分散処理　　：複数の端末にまたがって処理を行う
8. PySparkとは？
    * 分散処理を実現するエンジン
        * 一つの大きな仕事を処理するためのフレームワーク
    * PySparkの仕組み
        * ドライバー　　　：コントローラー
        * エグゼキューター：ワーカー
9. ノートブックとは？
    * GUIで視覚的に分析を可能にしたノートのような分析環境
10. Sparkがデータ操作で利用するもの
    * データ操作
        * SQL
        * DataFrame：コードチック
        * RDD
    * Sparkの2つの読み込みタイプ
        * スキーマオンリード：事前のテーブル定義がなくても、データ読み込み可能
        * スキーマオンライト：データを読み込むためには、事前のデータ定義が必要
11. データ読み込み
    * JSON, CSVデータの読み込み
        * inferSchema
        * lineSep
        * header
        * multiline
        * Schema
12. データフレームを操作する
    * データを枠組みとして扱うことができる
    * データフレームの作成方法
        * SQLから生成
        * データを読み込んで生成
        * プログラム的に作成
    * 操作例
        * withColumnとalias
        * When
        * fillna
        * filter
        * subtract
13. カラムナーフォーマット/行指向フォーマット
    * JSON, CSVなどのローデータはスプリッタブルなので扱いにくい
        * スプリッタブル：データを複数端末で分割して処理することが可能
        * そのためAvro（行指向フォーマット）形式に変換して処理
    * カラムナーフォーマット
        * Parquet
    * 行指向フォーマット
        * Avro
14. パーティションとダイナミックパーティション
    * パーティション：特定のデータ区切り
    * ダイナミックパーティション：データをもとに振り分ける
15. スモールファイルとデータスキュー
    * スモールファイル：データが小さすぎると処理のボトルネックになる
        * パーティションを分けすぎるなど
        * 多くのサーバーを立ち上げる必要があるため
    * データスキューネス
        * データの偏りが大きいと処理の遅延となる
            * パーティションの切り方など

### セクション3: PySpark基本操作（SQLとDataFrame）


### セクション4: 非構造データのラングリング（エクストラ）

### セクション5: ボーナスレクチャー
